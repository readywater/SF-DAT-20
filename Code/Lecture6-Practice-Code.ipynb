{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use today is **Boston Dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from sklearn import feature_selection\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ga-students/SF-DAT-20/master/Data/Boston.csv\"\n",
    "BostonData = pd.read_csv(url)\n",
    "del BostonData['Unnamed: 0']\n",
    "BostonData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston data frame has 506 rows and 14 columns.\n",
    "Usage\n",
    "\n",
    "Boston\n",
    "\n",
    "Format\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "crim\n",
    "\n",
    "    per capita crime rate by town \n",
    "    \n",
    "zn\n",
    "\n",
    "    proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "    \n",
    "indus\n",
    "\n",
    "    proportion of non-retail business acres per town \n",
    "    \n",
    "chas\n",
    "\n",
    "    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    \n",
    "nox\n",
    "\n",
    "    nitrogen oxides concentration (parts per 10 million) \n",
    "    \n",
    "rm\n",
    "\n",
    "    average number of rooms per dwelling \n",
    "    \n",
    "age\n",
    "\n",
    "    proportion of owner-occupied units built prior to 1940 \n",
    "    \n",
    "dis\n",
    "\n",
    "    weighted mean of distances to five Boston employment centres \n",
    "    \n",
    "rad\n",
    "\n",
    "    index of accessibility to radial highways \n",
    "    \n",
    "tax\n",
    "\n",
    "    full-value property-tax rate per 10,000 dollars\n",
    "    \n",
    "ptratio\n",
    "\n",
    "    pupil-teacher ratio by town \n",
    "    \n",
    "black\n",
    "\n",
    "    1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "    \n",
    "lstat\n",
    "\n",
    "    lower status of the population (percent) \n",
    "    \n",
    "medv\n",
    "\n",
    "    median value of owner-occupied homes in 1000 dollars\n",
    "\n",
    "Source\n",
    "\n",
    "Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.\n",
    "\n",
    "Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n",
    "[Package MASS version 7.2-29 Index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to predict the median value of properties (medv) based on other variables in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's draw a scatter-plot of medv and lstat. Intuitively, does it like a pure linear association or it seems like there is some sort of non-linearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's first define few non-linear terms. Start from a pure linear function and go up to polynomial degree 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "BostonData['lstat_3'] = BostonData['lstat']**3\n",
    "BostonData['lstat_4'] = BostonData['lstat']**4\n",
    "BostonData['lstat_5'] = BostonData['lstat']**5\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','lstat_2','lstat_3']]\n",
    "X4 = BostonData[['lstat','lstat_2','lstat_3','lstat_4']]\n",
    "X5 = BostonData[['lstat','lstat_2','lstat_3','lstat_4','lstat_5']]\n",
    "y = BostonData['medv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now divide your dataset into 25% test set and 75% training set and use Validation and MSE of test set to decide which degree of polynomial fits the best. Run this procedure a few times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, on the same data set, use 10 fold cross-validation to decide on the degree of polynomial. Justify what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's consider more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first focus on correlation Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's first get rid of additional variables we added to our dataframe\n",
    "del BostonData['lstat_2']\n",
    "del BostonData['lstat_3']\n",
    "del BostonData['lstat_4']\n",
    "del BostonData['lstat_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List 3 variables that have the highest chance to appear in your final model - the model that can predict medv. Can these variables appear simultaneously in your final model if your goal is interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's standardize our data and put it in a new DataFrame called BostonDataNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation and Lasso regression on our standardized data to decide which variables to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation to choose our best model among the following candidates. Let's first add lstat**2 to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','chas']]\n",
    "X4 = BostonData[['lstat','lstat_2','chas']] #'black' is highly correlated with lstat so cannot consider them simoltanously\n",
    "X5 = BostonData[['ptratio','chas']]\n",
    "X6 = BostonData[['ptratio','chas','black']]\n",
    "X7 = BostonData[['ptratio','black']]\n",
    "X8 = BostonData[['rm']]\n",
    "X9 = BostonData[['rm','chas']]\n",
    "X10 = BostonData[['rm','chas','black']]\n",
    "X11 = BostonData[['rm','black']]\n",
    "X12 = BostonData[['lstat','ptratio','rm']]  #model without that much interpretability\n",
    "X13 = BostonData[['lstat','lstat_2','ptratio','rm']]  #model without that much interpretability\n",
    "X14 = BostonData[['lstat','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "X15 = BostonData[['lstat','lstat_2','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "y = BostonData['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use 10 fold cross-validation to decide on the model of your interest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is interpretation - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is prediction - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [X12,X12,X14,X15]:\n",
    "    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n",
    "    print(lm1.summary())\n",
    "    \n",
    "    #All of our models are highly significant, so we use model 15. It generates the least CV-MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
