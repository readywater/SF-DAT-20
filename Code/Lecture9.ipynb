{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   admit  gre   gpa  prestige\n",
      "0      0  380  3.61         3\n",
      "1      1  660  3.67         3\n",
      "2      1  800  4.00         1\n",
      "3      1  640  3.19         4\n",
      "4      0  520  2.93         4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317380</td>\n",
       "      <td>587.858942</td>\n",
       "      <td>3.392242</td>\n",
       "      <td>2.488665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466044</td>\n",
       "      <td>115.717787</td>\n",
       "      <td>0.380208</td>\n",
       "      <td>0.947083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa    prestige\n",
       "count  397.000000  397.000000  397.000000  397.000000\n",
       "mean     0.317380  587.858942    3.392242    2.488665\n",
       "std      0.466044  115.717787    0.380208    0.947083\n",
       "min      0.000000  220.000000    2.260000    1.000000\n",
       "25%      0.000000  520.000000    3.130000    2.000000\n",
       "50%      0.000000  580.000000    3.400000    2.000000\n",
       "75%      1.000000  660.000000    3.670000    3.000000\n",
       "max      1.000000  800.000000    4.000000    4.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"../HW assignments/HW2/admissions.csv\"\n",
    "AdmissionData = pd.read_csv(url)\n",
    "print(AdmissionData.head(5))\n",
    "# Let's get rid of Missing values - there are only a few missing values so, let's drop them all\n",
    "AdmissionData.dropna(inplace = True)\n",
    "AdmissionData.describe() # Only 3 rows are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>prestige_1.0</th>\n",
       "      <th>prestige_2.0</th>\n",
       "      <th>prestige_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige  prestige_1.0  prestige_2.0  prestige_3.0\n",
       "0      0  380  3.61         3             0             0             1\n",
       "1      1  660  3.67         3             0             0             1\n",
       "2      1  800  4.00         1             1             0             0\n",
       "3      1  640  3.19         4             0             0             0\n",
       "4      0  520  2.93         4             0             0             0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrestigeDummy = pd.get_dummies(AdmissionData.prestige, prefix = 'prestige')\n",
    "del PrestigeDummy['prestige_4.0'] \n",
    "AdmissionData = pd.concat([AdmissionData, PrestigeDummy], axis=1)\n",
    "AdmissionData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = LogisticRegression()\n",
    "y = AdmissionData['admit']\n",
    "X = AdmissionData[['gre','gpa','prestige_1.0','prestige_2.0','prestige_3.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.58889206e-03   1.84630743e-04   1.16761197e+00   5.26947989e-01\n",
      "   -3.80822678e-02]]\n",
      "[-2.07018745]\n"
     ]
    }
   ],
   "source": [
    "lm.fit(X,y)\n",
    "print lm.coef_\n",
    "print lm.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we learn from these coefficients (assume they are all significant)?\n",
    "\n",
    "Higher GPA and higher GRE adds to odds of admission. The same story is true for prestige. The higher the prestige of your undergraduate school the more your chance of getting admitted. We are going to interpret the coefficients in more detail later.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In comparison to the person who is graduated from a tier 4 school (i.e. prestige = 4), what are the odds of a person who has graduated from a top level school to get admitted to UCLA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21430760315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.exp(1.16761197)  - 1)\n",
    "#Her odds of admission is 221% more than a person who is graduated from a \n",
    "#tier 4 school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student A and Student B are graduated from same school, have same background and same GPA. The only difference between the two is their GRE score. Student A's GPA is 50 scores better. What is the difference of odds of admission for these two students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0826860101613\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(50*0.0015889 ) - 1)\n",
    "#Student A have 8.26% better odds than student B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the probability of a student who came from a second tier school to get admitted to UCLA if her GPA is 3.5, GRE Score is 650."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.62476273  0.37523727]]\n"
     ]
    }
   ],
   "source": [
    "x1 = [[650,3.5,0,1,0]]\n",
    "print(lm.predict(x1))  #It is more likely that she will not get admitted\n",
    "print(lm.predict_proba(x1)) #Probability of Admission is only 37.5%\n",
    "# Two numbers: Probability that she won't, probability that she will.\n",
    "# [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's calculate cross-validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80487805  0.575       0.725       0.675       0.7         0.675\n",
      "  0.69230769  0.64102564  0.71794872  0.66666667]\n",
      "0.687282676673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "print(cross_val_score(lm,X,y,cv=10)) # Doesn't work when you have more than one var\n",
    "#cross_val_score(model,inputs,output,cv = k-fold).mean()\n",
    "print(cross_val_score(lm,X,y,cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256,  15],\n",
       "       [104,  22]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_hat = lm.predict(X)\n",
    "confusion_matrix(y, y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try testing and plotting CV Scores with different C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c1ff650>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEQCAYAAACwSgOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNHNoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102135f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Try testing and plo\n",
    "c_list = np.logspace(-10,10,21) \n",
    "c_index = np.linspace(-10,10,21)\n",
    "#C is just the inverse of Lambda - the smaller the C - the stronger the\n",
    "#regulatization. The smaller C's choose less variables\n",
    "cv_scores = []\n",
    "for c_score in c_list:\n",
    "    lm = LogisticRegression(C = c_score)\n",
    "    cv_scores.append(cross_val_score(lm,X,y,cv=10).mean()) # Remove the prev loop\n",
    "    # CV_score is crossval precision\n",
    "\n",
    "C_Choice_df = pd.DataFrame({'cv_scores': cv_scores ,'Log_C': c_index })\n",
    "C_Choice_df.plot(x ='Log_C',y = 'cv_scores' );\n",
    "# our choice is C = 10 becajuse we get Log(1) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_Choice_df.Log_C[C_Choice_df.cv_scores.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00208211  0.63232997  1.46818044  0.79139463  0.15518283]]\n"
     ]
    }
   ],
   "source": [
    "lm = LogisticRegression(C = 10)\n",
    "lm.fit(X,y)\n",
    "print(lm.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IF YOU WAN TO MAKE COEFFICIENTS RELEVANT - YOU MUST STANDARDIZE YOUR DATA FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Standardize(X):\n",
    "    X_Max = X.max()\n",
    "    X_Min = X.min()\n",
    "    X_Standardized = (X-X_Min)/(X_Max - X_Min)\n",
    "    return X_Standardized\n",
    "\n",
    "#Please try it yourself\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
