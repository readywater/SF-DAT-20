# SF-DAT-20		
**Instructor: Hamed Hasheminia**


Monday | Wednesday
--- | ---
-| 1/27: Data Science - Introduction Part I
2/1: Data Science - Introduction Part II | 2/3: Basic Statistics - Review Session
2/8: Linear Regression Lines - Part I | 2/10: Linear Regression Lines - Part II
2/15: No Class - Presidents' Day | 2/17: Model Selection
2/22: Missing Data and Imputation | 2/24: K-Nearest Neighbors
2/29: Logistic Regression Part I | 3/2: Logistic Regression Part II
3/7: Decision Trees Part I | 3/9: Decision Trees Part II
3/14: Natural Language Processing | 3/16: Principal Component Analysis
3/21: Time Series Models | 3/23: Databases and SQL
3/28: Naive Bayes | 3/30: Course Review
4/4: Final Project Presentations I | 4/6: Final Project Presentations II 



##Lecture 1 Summary (Data Science - Introduction Part I)

 - We talked about different roles of Data Scientists 
 - T-Shaped Data Scientists
 - Data Science Workflow
 - Continuous, Discrete and Qualitative Data
 - Supervised vs Unsupervised Learning
 - Set up github accounts
 - set ipython notebook
 - Introduced Numpy
 
**Resources**
 
 - [Lecture 1 - GitHub - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%201%20-%20GitHub.pptx) 
 - [Lecture 1 - Introduction - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%201%20-%20Introduction.pptx)
 - [Intro Numpy - Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Intro_Numpy.ipynb)
 - [Intro Numpy - Solution Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Intro_Numpy_Solution.ipynb)


## Lecture 2 Summary (Data Science - Introduction Part II)
- Classification vs Clustering and Regression vs Dimentionality Reduction
- Flexibility vs Interpretability
- Different types of data (Cross-Sectional, Time-Series, Panel Data) 
- Walkthrough Acquire & Parses with Pandas
- HW 1 assigned - Due date Feb 8th at 6:30PM

**Resources**
 
 - [Lecture 2 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%202%20.pptx) 
 - [Introduction to Numpy](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture2.ipynb) part II
 - [Lecture 2 - Lab Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture2-Practice-Code.ipynb)
 - [Lecture 2 - Lab Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture2-Practice-Solution.ipynb)

**Additional Resources**

Name | Description
--- | ---
[Official Pandas Tutorials](http://pandas.pydata.org/pandas-docs/stable/tutorials.html) | Wes & Company's selection of tutorials and lectures
[Julia Evans Pandas Cookbook](https://github.com/jvns/pandas-cookbook) | Great resource with examples from weather, bikes and 311 calls
[Learn Pandas Tutorials](https://bitbucket.org/hrojas/learn-pandas) | A great series of Pandas tutorials from Dave Rojas
[Research Computing Python Data PYNBs](https://github.com/ResearchComputing/Meetup-Fall-2013/tree/master/python) | A super awesome set of python notebooks from a meetup-based course exclusively devoted to panda
  
## Lecture 3 Summary (Basic Statistics - Review Session)
- Measures of central tendency (Mean, Median, Mode, Quartiles, Percentiles)- Measures of Variability (IQR, Standard Deviation, Variance)- Skewness Coefficient - Kurtosis Coefficient- Boxplots- Bias vs Variance- Central Limit Theorem â€“ Standard Error of Mean- Class/Dummy Variables
- Walkthrough describing and visualizing data in Pandas

**Resources**
 
 - [Lecture 3 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%203%20.pptx) 
 - [Basic Statistics Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture3.ipynb) 
 - [Basic Statistics Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture3-Practice-Code.ipynb)
 - [Basic Statistics - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture3-Practice-Solution.ipynb)

## Lecture 4 Summary (Linear Regression Lines - Part I)
- Linear Regression lines
- Single Variable and Multi-Variable Regression Lines
- Capture non-linearity using Linear Regression lines.
- Interpretting regression coefficients
- Dealing with dummy variables in regression lines
- intro on sklearn and searborn library
- HW 2 assigned - Due date Feb 17th 2016 at 6:30PM

**Resources**
 
- [Lecture 4 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%204.pptx) 
- [Linear Regression - Part I - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture4.ipynb) 
- [Linear Regression - Part I - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture4-Practice-Code.ipynb)
- [Linear Regression - Part I - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture4-Practice-Solution.ipynb)


## Lecture 5 Summary (Linear Regression Lines - Part II)
- Hypothesis test - test of significance on regression coefficients
- p-value
- Capture non-linearity using Linear Regression lines.
- Different types of errors and R-squared
- Interaction Effects

**Resources**
 
- [Lecture 5 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%205.pptx) 
- [Linear Regression - Part II - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture5.ipynb) 
- [Linear Regression - Part II - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture5-Practice-code.ipynb)
- [Linear Regression - Part II - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture5-Practice-Solution.ipynb)
	
## Lecture 6 Summary (Model Selection)

- Bias-Variance Trade off
- Validation (Test vs Train set)
- Cross-Validation
- Ridge and Lasso Regression
- (Optional) Backward Selection, Forward Selection, All Subset Selection. (If you want to use these methods you need to use R)

**Resources**
 
 - [Lecture 6 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%206.pptx) 
 - [Model Selection - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture6.ipynb) 
 - [Model Selection  - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture6-Practice-Code.ipynb) 
 - [Model Selection  - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture6-Practice-Solution.ipynb)

## Lecture 7 Summary (Missing Data and Imputation)
- Types of missing data (MCAR, MAR, NMAR)
- Single imputation and their limitations
- Imuptation using regression lines and error
- Hot deck imputation
- multiple imputation

**Resources**
 
- [Lecture 7 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%207.pptx) 
- [Missing Data and Imputation - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture7.ipynb) 
- [Missing Data and Imputation  - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture7-Practice-Code.ipynb) 
- [Missing Data and Imputation  - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture7-Practice-Solution.ipynb)

## Lecture 8 Summary (K-Nearest Neighbors)
- Classification Problems
- Misclassifciation Error
- KNN algorithm for Classification
- Cross-Validation for KNN Algorithm
- Limitations of KNN Algorithm
- KNN algorithm for Regression

**Resources**
 
- [Lecture 8 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%208.pptx) 
- [K-Nearest Neighbors - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture8.ipynb) 
- [K-Nearest Neighbors  - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture8-Practice-Code.ipynb) 
- [K-Nearest Neighbors  - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture8-Practice-Solution.ipynb)

## Lecture 9 Summary (Logistic Regression Part I)
- Intro to Logistic Regression
- Odds vs Probability
- Using Logistic Regression to Make predictions
- How one interprets coefficients of Logistic Regression model
- Strength and weaknesses of Logistic Regression Model

**Resources**
 
- [Lecture 9 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%209.pptx) 
- [Logistic Regression Part I - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture9.ipynb) 
- [Logistic Regression Part I - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture9-Practice-Code.ipynb) 
- [Logistic Regression Part I - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture9-Practice-Solutions.ipynb)

## Lecture 10 Summary (Logistic Regression Part II)
- Unbalanced observations and Logistic Regression
- FP/FN/TP/TN/FPR/TPR
- The effect of chaning Threshold
- ROC Curves
- Area Under Curve
- How to compare classifciation algorithms

**Resources**
 
- [Lecture 10 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2010.pptx)
- [Logistic Regression Part II - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture10.ipynb) 
- [Logistic Regression Part II - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture10-Practice-Code.ipynb) 
- [Logistic Regression Part II - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture10-Practice-Solution.ipynb)

## Lecture 11 Summary (Decision Trees Part I)
- Decision Tree for Regression
- Greedy Approach
- Decision Tree for Classification
- Gini Index and Entropy index
- Limitation of Simple Decision Tree

**Resources**
 
- [Lecture 11 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2011.pptx)
- [Decision Trees Part I - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture11.ipynb) 
- [Decision Trees Part I - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture11-Practice-Code.ipynb) 
- [Decision Trees Part I - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture11-Practice-Solution.ipynb)

## Lecture 12 Summary (Decision Trees Part II)
- Bagging
- Random Forest
- Boosting
- Tuning parameters for boosting and Random Forest

**Resources**
 
- [Lecture 12 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2012.pptx)
- [Decision Trees Part II - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture12.ipynb) 
- [Decision Trees Part II - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture12-Practice-Code.ipynb) 



**Additional Resources**

- [Decision Tree - Video - Part 1](https://www.youtube.com/watch?v=U-dYqlvafYA)
- [Decision Tree - Video - Part 2](https://www.youtube.com/watch?v=6fopQt_ENeU)
- [Decision Tree - Video - Part 3](https://www.youtube.com/watch?v=BaPmPEWxKu0)
- [BootStrap - Video](https://www.youtube.com/watch?v=8bLsk1WXgDk)
 

## Lecture 13 Summary (Natural Language Processing)
- Definition of Natural Language Processing 
- NLP applications
- Basic NLP practice
- Stop words, bag-of-words, IF-DIF

**Resources**
 
- [Lecture 13 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2013.pptx)
- [Natural Language Processing - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture13.ipynb) 
- [Natural Language Processing - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture13-Practice-Code.ipynb) 
- [Natural Language Processing - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture13-Practice-Solution.ipynb)


**Additional Resources**

* If you want to learn a lot more NLP, check out the excellent [video lectures](https://class.coursera.org/nlp/lecture) and [slides](http://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) from this [Coursera course](https://www.coursera.org/course/nlp) (which is no longer being offered).
* [Natural Language Processing with Python](http://www.nltk.org/book/) is the most popular book for going in-depth with the [Natural Language Toolkit](http://www.nltk.org/) (NLTK).
* [A Smattering of NLP in Python](https://github.com/charlieg/A-Smattering-of-NLP-in-Python/blob/master/A%20Smattering%20of%20NLP%20in%20Python.ipynb) provides a nice overview of NLTK, as does this [notebook from DAT5](https://github.com/justmarkham/DAT5/blob/master/notebooks/14_nlp.ipynb).
* [spaCy](http://spacy.io/) is a newer Python library for text processing that is focused on performance (unlike NLTK).
* If you want to get serious about NLP, [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml) is a suite of tools (written in Java) that is highly regarded.
* When working with a large text corpus in scikit-learn, [HashingVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick) is a useful alternative to CountVectorizer.
* [Automatically Categorizing Yelp Businesses](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) discusses how Yelp uses NLP and scikit-learn to solve the problem of uncategorized businesses.
* [Modern Methods for Sentiment Analysis](http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis) shows how "word vectors" can be used for more accurate sentiment analysis.
* [Identifying Humorous Cartoon Captions](http://www.cs.huji.ac.il/~dshahaf/pHumor.pdf) is a readable paper about identifying funny captions submitted to the New Yorker Caption Contest.

## Lecture 14 Summary (Principal Component Analysis)

- Principal Component Analysis
- Computation of PCAs
- Geometry of PCAs
- Proportion of Variance Explained

**Resources**
 
- [Lecture 14 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2014.pptx)
- [Principal Component Analysis - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture14.ipynb) 
- [Principal Component Analysis - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture14-Practice-Code.ipynb) 
- [Principal Component Analysis - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture14-Practice-Solution.ipynb)

**Additional Resources**

* [This tutorial](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) on Principal Components Analysis (PCA) includes good refreshers on covariance and linear algebra
* To go deeper on Singular Value Decomposition, read [Kirk Baker's excellent tutorial](https://www.ling.ohio-state.edu/%7Ekbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf).
* Chapter 10 of  [Statistical Learning with applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf)

## Lecture 15 Summary (Time Series Models)

- AutoRegressive Models
- Moving Averages
- ARMA
- ARIMA

**Resources**
 
- [Lecture 15 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2015.pptx)
- [Time Series Models - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture15.ipynb) 
- [Time Series Models - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture15-Practice-Code.ipynb) 
- [Time Series Models - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture15-Practice-Solution.ipynb)

**Additional Resources**

* [This is a good resource](http://www.maths.qmul.ac.uk/~bb/TS_Chapter4_5.pdf) for AR models
* [Relatively easy to read book](http://www.statistik-mathematik.uni-wuerzburg.de/fileadmin/10040800/user_upload/time_series/the_book/2012-August-01-times.pdf) on time series. 

## Lecture 16 Summary (Databases and SQL)

* Talked about databases and data warehouse design.
* Introduction to SQL and learn the Fundamental Growth Query.* Look at product engagement data of a fictional company and use FGQ to compute retention curves.* Apply convolution to the retention curve to project future active users.* Build a model to predict the retention likelihood of individual customers. 
* **Thanks to Michael**

**Resources**

- [Lecture 16 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2016.pdf) 
- [Databases and SQL - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture16.ipynb) 
- [Databases and SQL - Practice Solutions](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture16-Practice-Solution.ipynb)

**Additional Resources**

* [Well organized and easy to undestand tutorials](http://www.w3schools.com/sql/default.asp) - Thanks to Catherine
* More tutorials on [SQL](https://sqlschool.modeanalytics.com/the-basics/introduction/) - Thanks to Randy

## Lecture 17 Summary (Naive Bayes)

**Pre-Work**

* Please review [Bayes Questions](https://github.com/ga-students/SF-DAT-20/blob/master/Resources%20for%20Students/BayesExamples.pdf).

**Summary**

* Naive Bayes Algorithm introduced
* Guassian NB
* Bernoulli NB
* Multinomial NB
* Advantages and Disadvantages of using NB

**Resources**

- [Lecture 17 - Slides](https://github.com/ga-students/SF-DAT-20/blob/master/Lecture%20Notes-Slides/Lecture%2017.pptx)
- [Naive Bayes - Lab Codes](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture17.ipynb) 
- [Naive Bayes - Practice Code](https://github.com/ga-students/SF-DAT-20/blob/master/Code/Lecture17-Practice-Code.ipynb) 


**Additional Resources**

* High level overview of Naive Bayes - [Useful slides](http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf)  
* Easy-to-read lecture notes on [Naive Bayes](http://www.cs.colostate.edu/~cs545/fall13/dokuwiki/lib/exe/fetch.php?media=wiki%3A13_naive_bayes.pdf)
* Naive Bayes model explained in more detail. [Harvard Unviersity - lecture notes](http://isites.harvard.edu/fs/docs/icb.topic540049.files/cs181_lec18_handout.pdf)
* For more details on Naive Bayes classification, Wikipedia has two excellent articles ([Naive Bayes classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) and [Naive Bayes spam filtering](http://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)), and Cross Validated has a good [Q&A](http://stats.stackexchange.com/questions/21822/understanding-naive-bayes).

